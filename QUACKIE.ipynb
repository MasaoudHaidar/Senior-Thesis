{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f991f2e5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c734661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.22.2\n",
    "\n",
    "!pip install statsmodels\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "!pip install -U tensorflow==2.10 \n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "import spacy\n",
    "import re\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, mean_absolute_percentage_error, r2_score, jaccard_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# specific machine learning functionality\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Transformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    TFBertForSequenceClassification, \n",
    "    TFBertForMaskedLM, \n",
    "    TFBertModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38980666",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dir = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2627fb22",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset = datasets.load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cf340",
   "metadata": {},
   "source": [
    "# Labeling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_train = qadataset['train']\n",
    "qadataset_test = qadataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_negative(example):\n",
    "    context = example['context']\n",
    "    answer_start = example['answers']['answer_start'][0]\n",
    "    sentence_number = context[:answer_start].count(\".\")\n",
    "    sentences = context.split(\".\")\n",
    "    example['context'] = '.'.join(sentences[:sentence_number] + sentences[sentence_number+1:])\n",
    "    example['label'] = False\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_positive(example):\n",
    "    example['label'] = True\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a541a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_train_label_split = qadataset_train.train_test_split(test_size=0.5, shuffle=True, seed=109)\n",
    "\n",
    "qadataset_train_positive = qadataset_train_label_split['train']\n",
    "qadataset_train_negative = qadataset_train_label_split['test']\n",
    "\n",
    "qadataset_train_negative = qadataset_train_negative.map(make_negative)\n",
    "qadataset_train_positive = qadataset_train_positive.map(make_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test_label_split = qadataset_test.train_test_split(test_size=0.5, shuffle=True, seed=109)\n",
    "\n",
    "qadataset_test_positive = qadataset_test_label_split['train']\n",
    "qadataset_test_negative = qadataset_test_label_split['test']\n",
    "\n",
    "qadataset_test_positive = qadataset_test_positive.map(make_positive)\n",
    "qadataset_test_negative = qadataset_test_negative.map(make_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e09c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_train = datasets.concatenate_datasets([qadataset_train_positive, qadataset_train_negative])\n",
    "qadataset_test = datasets.concatenate_datasets([qadataset_test_positive, qadataset_test_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_q_a(example):\n",
    "    example['text'] = '[CLS] ' + example['question'] + ' [SEP] ' + example['context']\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_train = qadataset_train.map(combine_q_a)\n",
    "qadataset_test = qadataset_test.map(combine_q_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14025d4a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization parameters\n",
    "classifier_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(classifier_name, do_lower_case=True)\n",
    "batch_size = 8 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c04f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization function\n",
    "def tokenize_for_bert_classifier(df, should_shuffle=False):\n",
    "    # Tokenization\n",
    "    X_tokenized = bert_tokenizer.batch_encode_plus(\n",
    "            df[\"text\"],\n",
    "            return_tensors='tf',\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_attention_mask = True,\n",
    "            truncation='longest_first'\n",
    "    )\n",
    "    # Creating TF datasets\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((X_tokenized[\"input_ids\"],\n",
    "                                                   X_tokenized[\"token_type_ids\"],\n",
    "                                                   X_tokenized[\"attention_mask\"]), \n",
    "                                                  df[\"label\"]))\n",
    "    if should_shuffle:\n",
    "        buffer_train = len(df[\"text\"])\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_train)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15052733",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_train = qadataset_train.train_test_split(test_size=0.2, shuffle=True, seed=109)\n",
    "\n",
    "qadataset_train['validation'] = qadataset_train.pop('test')\n",
    "\n",
    "classification_training_data = tokenize_for_bert_classifier(qadataset_train['train'], should_shuffle=True)\n",
    "classification_validation_data = tokenize_for_bert_classifier(qadataset_train['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test_data = tokenize_for_bert_classifier(qadataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c6fceb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044257d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = TFBertForSequenceClassification.from_pretrained(word_dir + 'Senior Thesis models/model_classifier_bert_6/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cba9e8",
   "metadata": {},
   "source": [
    "# QUACKIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd54c7b",
   "metadata": {},
   "source": [
    "## Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_test_data = tokenize_for_bert_classifier(qadataset_test)\n",
    "Y_pred = classifier_model.predict(classification_test_data)\n",
    "Y_pred = Y_pred['logits'] > 0\n",
    "Y_pred_flat = [temp[0] for temp in Y_pred]\n",
    "qadataset_test = qadataset_test.add_column(\"predicted label\", Y_pred_flat)\n",
    "\n",
    "qadataset_test_TP = qadataset_test.filter(lambda example: example[\"label\"] == example[\"predicted label\"])\n",
    "\n",
    "qadataset_test_TP = qadataset_test_TP.filter(lambda example: example[\"label\"] == True)\n",
    "\n",
    "qadataset_test_TP = qadataset_test_TP.filter(lambda example: len(np.unique(example[\"answers\"][\"answer_start\"])) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test_TP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249be985",
   "metadata": {},
   "source": [
    "## Creat Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aceb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_re = r'[.]'\n",
    "def get_ground_truth_sentence_positions(example):\n",
    "    context = example['context']\n",
    "    answer_letter_start = example['answers']['answer_start'][0]\n",
    "    sentence_number = context[:answer_letter_start].count(\". \")\n",
    "    # sentence_number = len(re.findall(split_re, context[:answer_letter_start]))\n",
    "    example[\"answer_sentence_position\"] = sentence_number\n",
    "    return example\n",
    "\n",
    "qadataset_test_TP = qadataset_test_TP.map(get_ground_truth_sentence_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test_TP[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532a39d",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_tokens(example, classifier_model = classifier_model):\n",
    "    text = example[\"text\"]\n",
    "    question_length = len(example['question'].split()) + 2\n",
    "\n",
    "    _, full_replacement_list = show_multiple_masking_replacement_score(\n",
    "      text, \n",
    "      classifier = classifier_model,\n",
    "      n_samples_per_word = 1,\n",
    "      return_type=\"list\",\n",
    "      ignore_first_x_words = question_length\n",
    "    )\n",
    "    full_replacement_list = full_replacement_list[question_length + 1:]\n",
    "\n",
    "    _, full_masking_list = show_multiple_masking_score(\n",
    "      text, \n",
    "      classifier = classifier_model,\n",
    "      n_samples_per_word = 5,\n",
    "      return_type=\"list\",\n",
    "      ignore_first_x_words = question_length\n",
    "    )\n",
    "    full_masking_list = full_masking_list[question_length + 1:]\n",
    "\n",
    "    example[\"predicted_full_sentence_scores_by_replacement\"] = full_replacement_list\n",
    "    example[\"predicted_full_sentence_scores_by_masking\"] = full_masking_list\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_sentence_scores(text, score_list):\n",
    "    aggregated_scores = []\n",
    "    sentences = text.split(\". \")\n",
    "    #sentences = re.split(split_re, text)\n",
    "    if sentences[-1] == \"\":\n",
    "        sentences = sentences[:-1]\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "        sentence_scores =  np.asarray(score_list[:sentence_length])\n",
    "        # current_score = np.mean(sentence_scores)\n",
    "        # x[x > -np.percentile(-x, 10)]\n",
    "        # current_score = np.mean(sentence_scores[sentence_scores > -np.percentile(-sentence_scores, 20)])\n",
    "        top_words = 4\n",
    "        # top_words = max(2, int(len(sentence_scores) * 0.2)) # 20%\n",
    "        if len(sentence_scores) < top_words + 1:\n",
    "            current_score = np.mean(sentence_scores)\n",
    "        else:\n",
    "            current_score = np.mean(np.partition(sentence_scores, -top_words)[-top_words:])\n",
    "        # current_score = np.mean(sentence_scores)\n",
    "        aggregated_scores.append(current_score)\n",
    "\n",
    "        score_list = score_list[sentence_length:]\n",
    "    return aggregated_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_sentence(example, classifier_model = classifier_model):\n",
    "    context = example[\"context\"]\n",
    "    full_replacement_list = example[\"predicted_full_sentence_scores_by_replacement\"]\n",
    "    full_masking_list = example[\"predicted_full_sentence_scores_by_masking\"]\n",
    "\n",
    "    replacement_list = aggregate_sentence_scores(context, full_replacement_list)\n",
    "    masking_list = aggregate_sentence_scores(context, full_masking_list)\n",
    "\n",
    "    example[\"predicted_answer_sentence_position_by_replacement\"] = np.argmax(replacement_list)\n",
    "    example[\"predicted_answer_sentence_position_by_masking\"] = np.argmax(masking_list)\n",
    "\n",
    "    example[\"predicted_sentence_scores_by_replacement\"] = replacement_list\n",
    "    example[\"predicted_sentence_scores_by_masking\"] = masking_list\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf117b",
   "metadata": {},
   "source": [
    "## Evaluate on Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d76038",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence_interpreters_by_accuracy(example): # is this IoU?\n",
    "    example[\"masking_accuracy\"] = (\n",
    "      example[\"predicted_answer_sentence_position_by_masking\"] ==\n",
    "      example[\"answer_sentence_position\"]\n",
    "    )\n",
    "    example[\"replacement_accuracy\"] = (\n",
    "      example[\"predicted_answer_sentence_position_by_replacement\"] ==\n",
    "      example[\"answer_sentence_position\"]\n",
    "    )\n",
    "    return example\n",
    "\n",
    "def evaluate_sentence_interpreters_by_snr(example): \n",
    "    def snr(gt_score, non_gt_scores):\n",
    "        return ((gt_score - np.mean(non_gt_scores)) ** 2) / np.std(non_gt_scores)\n",
    "\n",
    "    answer_position = example[\"answer_sentence_position\"]\n",
    "    example[\"masking_snr\"] = snr(\n",
    "      example[\"predicted_sentence_scores_by_masking\"][answer_position],\n",
    "      example[\"predicted_sentence_scores_by_masking\"][:answer_position] +\n",
    "      example[\"predicted_sentence_scores_by_masking\"][answer_position + 1:]\n",
    "    )\n",
    "    example[\"replacement_snr\"] = snr(\n",
    "      example[\"predicted_sentence_scores_by_replacement\"][answer_position],\n",
    "      example[\"predicted_sentence_scores_by_replacement\"][:answer_position] +\n",
    "      example[\"predicted_sentence_scores_by_replacement\"][answer_position + 1:]\n",
    "    )\n",
    "    return example\n",
    "\n",
    "def evaluate_sentence_interpreters_by_hpd(example): \n",
    "    def hpd(scores, correct_position):\n",
    "        return 1 / np.sum(np.asarray(scores) >= scores[correct_position])\n",
    "\n",
    "    answer_position = example[\"answer_sentence_position\"]\n",
    "    example[\"masking_hpd\"] = hpd(\n",
    "      example[\"predicted_sentence_scores_by_masking\"],\n",
    "      answer_position\n",
    "    )\n",
    "    example[\"replacement_hpd\"] = hpd(\n",
    "      example[\"predicted_sentence_scores_by_replacement\"],\n",
    "      answer_position\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b97e5",
   "metadata": {},
   "source": [
    "### Generate Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc07a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(109)\n",
    "sample_indices = range(200) # np.random.choice(range(2784), 100, replace=False)\n",
    "test_sample = qadataset_test_TP.select(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e809d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_sample.map(get_predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_sample.map(get_predicted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c14ddd",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_sample.map(evaluate_sentence_interpreters_by_accuracy)\n",
    "\n",
    "test_sample = test_sample.map(evaluate_sentence_interpreters_by_snr)\n",
    "\n",
    "test_sample = test_sample.map(evaluate_sentence_interpreters_by_hpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2020d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "masking_mean_accuracy = np.mean(test_sample[\"masking_accuracy\"])\n",
    "print(f\"Masking Accuracy: {masking_mean_accuracy}\")\n",
    "\n",
    "replacement_mean_accuracy = np.mean(test_sample[\"replacement_accuracy\"])\n",
    "print(f\"Replacement Accuracy: {replacement_mean_accuracy}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# NaNs and infs occur if there is only one or two sentences in the context because we can't estimate the std of the incorrect sentences. \n",
    "masking_mean_snr = np.mean(np.ma.masked_invalid(test_sample[\"masking_snr\"]))\n",
    "print(f\"Masking SNR: {masking_mean_snr}\")\n",
    "\n",
    "replacement_mean_snr = np.mean(np.ma.masked_invalid(test_sample[\"replacement_snr\"]))\n",
    "print(f\"Replacement SNR: {replacement_mean_snr}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "masking_mean_accuracy = np.mean(test_sample[\"masking_hpd\"])\n",
    "print(f\"Masking HPD: {masking_mean_accuracy:0.4f}\")\n",
    "\n",
    "replacement_mean_accuracy = np.mean(test_sample[\"replacement_hpd\"])\n",
    "print(f\"Replacement HPD: {replacement_mean_accuracy:0.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237eb8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
