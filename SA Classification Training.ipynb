{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267c4b6e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26efea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12beee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.22.2\n",
    "\n",
    "!pip install statsmodels\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "!pip install -U tensorflow==2.10 \n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f777bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "import spacy\n",
    "import re\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, mean_absolute_percentage_error, r2_score, jaccard_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# specific machine learning functionality\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Transformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    TFBertForSequenceClassification, \n",
    "    TFBertForMaskedLM, \n",
    "    TFBertModel,\n",
    "    #create_optimizer,\n",
    "    #DataCollatorForLanguageModeling,\n",
    "    #PreTrainedTokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a74495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dir = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f543eec2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286df491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"data/IMDB Dataset.csv\")\n",
    "df = pd.read_csv(word_dir + \"Colab Notebooks/IMDB Dataset.csv\") \n",
    "df = df.rename(columns={\"review\":\"text\", \"sentiment\": \"label\"})\n",
    "df[\"label\"] = df[\"label\"] == \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Positive Rate: {np.mean(df.label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_len = lambda s: len(s.split())\n",
    "lengths = df[\"text\"].apply(get_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a997cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths)\n",
    "plt.xlabel(\"Text length\", fontsize = 20)\n",
    "plt.ylabel(\"Frequency\", fontsize = 20)\n",
    "plt.gcf().set_size_inches(15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of data points of lengths high than 256: {sum(lengths > 256)}\")\n",
    "print(f\"Number of data points of lengths high than 512: {sum(lengths > 512)}\")\n",
    "print(f\"Number of data points of lengths high than 1024: {sum(lengths > 1024)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gap_filler, df_classification = train_test_split(df, test_size=0.5, random_state=1, stratify=df[\"label\"])\n",
    "print(f\"Positive Rate in Gap filler data: {np.mean(df_gap_filler.label)}\")\n",
    "print(f\"Positive Rate in Classifier data: {np.mean(df_classification.label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccefb9",
   "metadata": {},
   "source": [
    "# Train SA Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333938e",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization parameters\n",
    "classifier_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(classifier_name, do_lower_case=True)\n",
    "batch_size = 8 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0352357",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization function\n",
    "def tokenize_for_bert_classifier(df, should_shuffle=False):\n",
    "    # Tokenization\n",
    "    X_tokenized = bert_tokenizer.batch_encode_plus(\n",
    "            df[\"text\"],\n",
    "            return_tensors='tf',\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_attention_mask = True,\n",
    "            truncation='longest_first'\n",
    "    )\n",
    "    # Creating TF datasets\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((X_tokenized[\"input_ids\"],\n",
    "                                                   X_tokenized[\"token_type_ids\"],\n",
    "                                                   X_tokenized[\"attention_mask\"]), \n",
    "                                                  df[\"label\"]))\n",
    "    if should_shuffle:\n",
    "      buffer_train = len(df[\"text\"])\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_train)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classification_train_all, df_classification_test = train_test_split(df_classification, \n",
    "                                                                   test_size=0.2, \n",
    "                                                                   random_state=1, \n",
    "                                                                   stratify=df_classification[\"label\"])\n",
    "\n",
    "df_classification_train, df_classification_val = train_test_split(df_classification_train_all, \n",
    "                                                                  test_size=0.2, \n",
    "                                                                  shuffle=True, \n",
    "                                                                  random_state=1, \n",
    "                                                                  stratify=df_classification_train_all[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ed138",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_training_data = tokenize_for_bert_classifier(df_classification_train, should_shuffle=True)\n",
    "classification_validation_data = tokenize_for_bert_classifier(df_classification_val)\n",
    "classification_test_data = tokenize_for_bert_classifier(df_classification_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6456eb0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2343c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BERT Setup\n",
    "learning_rate = 2e-5\n",
    "epochs = 5\n",
    "def get_bert_classifier():\n",
    "    return TFBertForSequenceClassification.from_pretrained(classifier_name, \n",
    "                                                           num_labels=1, from_pt = True)\n",
    "\n",
    "def get_compiled_bert_classifier(model = None):\n",
    "    # Free up memory\n",
    "    K.clear_session()\n",
    "\n",
    "    # Build the model\n",
    "    if model is None:\n",
    "      model = get_bert_classifier()\n",
    "\n",
    "    # Print the model architecture\n",
    "    print(model.summary())\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "    # Loss\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss=loss,\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778553e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = get_compiled_bert_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train model\n",
    "train_model = False\n",
    "if train_model:\n",
    "    start_time = time.time()\n",
    "    training_results = classifier_model.fit(\n",
    "            classification_training_data,\n",
    "            validation_data=classification_validation_data,\n",
    "            epochs=epochs,\n",
    "            verbose=1)\n",
    "    execution_time = (time.time() - start_time)/60.0\n",
    "    print(\"Training execution time (mins)\",execution_time)\n",
    "    classifier_model.save_pretrained(word_dir + 'Senior Thesis models/model_classifier_bert_1/temp')\n",
    "else:\n",
    "    classifier_model = TFBertForSequenceClassification.from_pretrained(word_dir + 'Senior Thesis models/model_classifier_bert_1/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1e053",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation Function\n",
    "def evaluate_bert_classifier(bert_model, dataset, Y_true, only_accuracy = False):\n",
    "    Y_pred = bert_model.predict(dataset)\n",
    "    Y_pred = Y_pred['logits'] > 0\n",
    "    acc = accuracy_score(Y_true, Y_pred)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    if only_accuracy:\n",
    "        return\n",
    "    f1 = f1_score(Y_true, Y_pred)\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    recall = recall_score(Y_true, Y_pred)\n",
    "    print(f\"Recall score: {recall}\")\n",
    "    precision = precision_score(Y_true, Y_pred)\n",
    "    print(f\"Precision score: {precision}\")\n",
    "    \n",
    "    Y_pred = np.asarray([x[0] for x in Y_pred])\n",
    "    \n",
    "    TN = np.sum((Y_true == Y_pred) & (Y_pred == 0))\n",
    "    TP = np.sum((Y_true == Y_pred) & (Y_pred == 1))\n",
    "    \n",
    "    FN = np.sum((Y_true != Y_pred) & (Y_pred == 0))\n",
    "    FP = np.sum((Y_true != Y_pred) & (Y_pred == 1))\n",
    "    \n",
    "    print(f\"TN: {TN}, TP:{TP}, FN:{FN}, FP:{FP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1928b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must have should_shuffle=False \n",
    "evaluate_bert_classifier(classifier_model, classification_training_data, df_classification_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79abaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_bert_classifier(classifier_model, classification_validation_data, qadataset_train['validation'][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcae14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_bert_classifier(classifier_model, classification_test_data, df_classification_test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54588f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
