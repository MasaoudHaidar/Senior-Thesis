{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267c4b6e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26efea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12beee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.22.2\n",
    "\n",
    "!pip install statsmodels\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "!pip install -U tensorflow==2.10 \n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f777bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "import spacy\n",
    "import re\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, mean_absolute_percentage_error, r2_score, jaccard_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# specific machine learning functionality\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Transformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    TFBertForSequenceClassification, \n",
    "    TFBertForMaskedLM, \n",
    "    TFBertModel,\n",
    "    #create_optimizer,\n",
    "    #DataCollatorForLanguageModeling,\n",
    "    #PreTrainedTokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a74495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dir = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20b91a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization parameters\n",
    "classifier_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(classifier_name, do_lower_case=True)\n",
    "batch_size = 8 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0352357",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization function\n",
    "def tokenize_for_bert_classifier(df, should_shuffle=False):\n",
    "    # Tokenization\n",
    "    X_tokenized = bert_tokenizer.batch_encode_plus(\n",
    "            df[\"text\"],\n",
    "            return_tensors='tf',\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_attention_mask = True,\n",
    "            truncation='longest_first'\n",
    "    )\n",
    "    # Creating TF datasets\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((X_tokenized[\"input_ids\"],\n",
    "                                                   X_tokenized[\"token_type_ids\"],\n",
    "                                                   X_tokenized[\"attention_mask\"]), \n",
    "                                                  df[\"label\"]))\n",
    "    if should_shuffle:\n",
    "      buffer_train = len(df[\"text\"])\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_train)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe64a2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = TFBertForSequenceClassification.from_pretrained(word_dir + 'Senior Thesis models/model_classifier_bert_6/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a74c8e",
   "metadata": {},
   "source": [
    "# Single Masking Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e2adf",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8be381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "gap_untuned_model = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "to_keep = [\n",
    "    \"n't\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    'noone',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'nothing',\n",
    "    'n‘t',\n",
    "    'n’t',\n",
    "    'only',\n",
    "    'quite',\n",
    "    'really',\n",
    "    'serious',\n",
    "    'several',\n",
    "    'still',\n",
    "    'such',\n",
    "    'take',\n",
    "    'too',\n",
    "    'top',\n",
    "    'unless',\n",
    "    'various',\n",
    "    'very',\n",
    "    'well',\n",
    "]\n",
    "all_stopwords = [word for word in all_stopwords if not word in to_keep]\n",
    "all_stopwords = set(all_stopwords)\n",
    "letter_regex = re.compile('[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_replacement_options_and_score(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    gap_filler = gap_untuned_model,\n",
    "  ):\n",
    "    # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    word_scores = defaultdict(list)\n",
    "    for i, word in tqdm(enumerate(all_words), total = min(256, len(all_words))):\n",
    "        if i > 256:\n",
    "            break\n",
    "        word = letter_regex.sub(\"\", word)\n",
    "        word = word.lower()\n",
    "        if word in all_stopwords:\n",
    "            continue\n",
    "    \n",
    "    new_sentence = \" \".join([temp_word if j!=i else \"[MASK]\" for (j, temp_word) in enumerate(all_words)])\n",
    "    # new_sentence = original_sentence.replace(word, \"[MASK]\")\n",
    "    inputs = bert_tokenizer(new_sentence, return_tensors=\"tf\")\n",
    "    logits = gap_filler(**inputs).logits\n",
    "\n",
    "    # retrieve index of [MASK]\n",
    "    mask_token_index = tf.where((inputs.input_ids == bert_tokenizer.mask_token_id)[0])\n",
    "    selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "    \n",
    "    # get top predictions\n",
    "    predicted_token_ids = tf.math.top_k(selected_logits, 10).indices[0]\n",
    "    options = bert_tokenizer.decode(predicted_token_ids)\n",
    "\n",
    "    if verbose:\n",
    "        print(new_sentence)\n",
    "        print(options)\n",
    "\n",
    "    # get scores of those predictions\n",
    "    filled_sentences = []\n",
    "    for filler_word in options.split():\n",
    "        new_filled_sentece = original_sentence.replace(word, filler_word)\n",
    "        filled_sentences.append(new_filled_sentece)\n",
    "    \n",
    "    # compute word importance:\n",
    "    try:\n",
    "        inputs = bert_tokenizer(\n",
    "          filled_sentences, \n",
    "          return_tensors=\"tf\",\n",
    "          padding=True,\n",
    "          #max_length=256,\n",
    "          truncation=True\n",
    "        )\n",
    "        logits = classifier(**inputs).logits\n",
    "        current_word_score = original_sentence_score - np.mean(logits[:,0].numpy())\n",
    "        word_scores[word].append(current_word_score)\n",
    "        if verbose:\n",
    "            print(f\"Sentence Scores: {logits[:,0].numpy()}\")\n",
    "            print(f\"Overall Score: {(np.mean(logits[:,0].numpy())):.4f}\")\n",
    "            print(f\"Word: {word}\")\n",
    "            print(f\"Word importance: {current_word_score:.4f}\")\n",
    "            print()\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"Did not compute\")\n",
    "        word_importance_df = pd.DataFrame(\n",
    "          {\n",
    "              \"word\": word_scores.keys(),\n",
    "              \"importance\": [np.mean(temp) for temp in word_scores.values()]\n",
    "          }\n",
    "        )\n",
    "    word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "    return word_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_masking_score(\n",
    "    original_sentence, \n",
    "    verbose = False,\n",
    "    classifier = classifier_model,\n",
    "  ):\n",
    "    # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    word_scores = defaultdict(list)\n",
    "    for i, word in tqdm(enumerate(all_words), total = min(256, len(all_words))):\n",
    "        if i > 256:\n",
    "            break\n",
    "        word = letter_regex.sub(\"\", word)\n",
    "        word = word.lower()\n",
    "        if word in all_stopwords:\n",
    "            continue\n",
    "        new_sentence = \" \".join([temp_word if j!=i else \"\" for (j, temp_word) in enumerate(all_words)])\n",
    "        if verbose:\n",
    "            print(new_sentence)\n",
    "    \n",
    "    # compute word importance:\n",
    "    try:\n",
    "        inputs = bert_tokenizer(new_sentence, return_tensors=\"tf\")\n",
    "        logits = classifier(**inputs).logits\n",
    "        current_word_score = original_sentence_score - np.mean(logits[:,0].numpy())\n",
    "        word_scores[word].append(current_word_score)\n",
    "        if verbose:\n",
    "            print(f\"Sentence Scores: {logits[:,0].numpy()}\")\n",
    "            print(f\"Overall Score: {(np.mean(logits[:,0].numpy())):.4f}\")\n",
    "            print(f\"Word: {word}\")\n",
    "            print(f\"Word importance: {current_word_score:.4f}\")\n",
    "            print()\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"Did not compute\")\n",
    "        word_importance_df = pd.DataFrame(\n",
    "          {\n",
    "              \"word\": word_scores.keys(),\n",
    "              \"importance\": [np.mean(temp) for temp in word_scores.values()]\n",
    "          }\n",
    "        )\n",
    "    word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "    return word_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c62035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(df):\n",
    "    x = df.copy()\n",
    "    for i, row in df.iterrows():\n",
    "        if row[\"importance\"] > 0:\n",
    "            green_value = min(max(0, row[\"importance\"]*80), 256)\n",
    "            style = f'background-color: rgb({256 - green_value}, 256, {256 - green_value})'\n",
    "        else:\n",
    "            red_value = min(max(0, -row[\"importance\"]*80), 256)\n",
    "            style = f'background-color: rgb(256, {256 - red_value}, {256 - red_value})'\n",
    "        x.iloc[i] = style\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_both_scores(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    gap_models = [gap_untuned_model],\n",
    "    descriptions = [\"Replacement with un-tuned bert\"]\n",
    "  ):\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier_model(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    print(f\"Sentence score: {inv_logit(original_sentence_score):0.4F}\")\n",
    "    masking_df = show_masking_score(original_sentence, verbose, classifier)\n",
    "    replacement_dfs = [\n",
    "      show_replacement_options_and_score(original_sentence, \n",
    "                                         verbose,\n",
    "                                         classifier,\n",
    "                                         gap_model)\n",
    "      for gap_model in gap_models\n",
    "    ]\n",
    "    print(\"Baseline:\")\n",
    "    display(masking_df.style.apply(get_color, axis=None))\n",
    "    for description, replacement_df in zip(descriptions, replacement_dfs):\n",
    "        print(description)\n",
    "        display(replacement_df.style.apply(get_color, axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589e087",
   "metadata": {},
   "source": [
    "# Multi-Masking Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcf795",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alpha = 0.01\n",
    "top_k_words = 10\n",
    "def show_multiple_masking_replacement_score(\n",
    "        original_sentence, \n",
    "        verbose = False, \n",
    "        classifier = classifier_model,\n",
    "        gap_filler = gap_untuned_model,\n",
    "        n_samples_per_word = 2,\n",
    "        return_type=\"table\", # table, list, or both\n",
    "        ignore_first_x_words=0,\n",
    "    ):\n",
    "    # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    n_samples = len(all_words) * n_samples_per_word\n",
    "    word_scores = defaultdict(list)\n",
    "    X = []\n",
    "    Y = []\n",
    "    # replacement_size = int(np.sqrt(len(all_words)) + 1)\n",
    "    replacement_size = int(len(all_words) * 0.15 + 1)\n",
    "    sentences = []\n",
    "    for _ in tqdm(range(n_samples), total = n_samples):\n",
    "        # Sample masking indices\n",
    "        word_indices = np.random.choice(\n",
    "            range(ignore_first_x_words, len(all_words)), \n",
    "            size=replacement_size,\n",
    "            replace = False,\n",
    "        )\n",
    "        current_x_row = np.ones(len(all_words))\n",
    "        for i in word_indices:\n",
    "            current_x_row[i] = 0\n",
    "        for _ in range(top_k_words):\n",
    "            X.append(current_x_row)\n",
    "        words = [all_words[i] for i in word_indices]\n",
    "        words = [letter_regex.sub(\"\", word).lower() for word in words]\n",
    "        new_sentence = \" \".join([temp_word if j not in word_indices else \"[MASK]\" for (j, temp_word) in enumerate(all_words)])\n",
    "\n",
    "        # get gap filler logits\n",
    "        inputs = bert_tokenizer(new_sentence, return_tensors=\"tf\")\n",
    "        logits = gap_filler(**inputs).logits\n",
    "\n",
    "        # retrieve indices of [MASK]\n",
    "        mask_token_index = tf.where((inputs.input_ids == bert_tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "\n",
    "        # get top predictions\n",
    "        predicted_token_ids = [tf.math.top_k(temp, top_k_words).indices for temp in selected_logits]\n",
    "        options = [bert_tokenizer.decode(temp) for temp in predicted_token_ids]\n",
    "        options = [temp.split() for temp in options]\n",
    "        options = [temp if len(temp) == top_k_words \n",
    "                   else temp + [\"\" for _ in range(top_k_words - len(temp))]\n",
    "                   for temp in options]\n",
    "\n",
    "        # get scores of those predictions\n",
    "        filled_sentences = [new_sentence for _ in range(top_k_words)]\n",
    "        for i in range(top_k_words):\n",
    "            for j in range(replacement_size):\n",
    "                filled_sentences[i] = filled_sentences[i].replace(\"[MASK]\", options[j][i], 1)\n",
    "            sentences.append(filled_sentences[i])\n",
    "    \n",
    "    # compute model outcomes:\n",
    "    dataset = tokenize_for_bert_classifier(\n",
    "      pd.DataFrame({\n",
    "          \"text\": sentences,\n",
    "          \"label\": [True for _ in sentences]\n",
    "      })\n",
    "    )\n",
    "    Y = classifier.predict(dataset).logits\n",
    "  \n",
    "    # Train a simple model on the local data\n",
    "    simple_model = Lasso(lasso_alpha).fit(X, Y)\n",
    "  \n",
    "    if return_type == \"table\" or return_type == \"both\":\n",
    "        filtered_words = list(filter(lambda w: w.lower() not in all_stopwords, all_words))\n",
    "        all_words_unique = [letter_regex.sub(\"\", word).lower() for word in filtered_words]\n",
    "        all_words_unique = list(set(all_words_unique))\n",
    "        word_importance_raw = defaultdict(list)\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_raw[letter_regex.sub(\"\", word).lower()].append(simple_model.coef_[i])\n",
    "        word_importance_df = pd.DataFrame(\n",
    "            {\n",
    "                \"word\": all_words_unique,\n",
    "                \"importance\": [np.mean(word_importance_raw[temp]) for temp in all_words_unique]\n",
    "            }\n",
    "        )\n",
    "        word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "\n",
    "    if return_type == \"list\" or return_type == \"both\":\n",
    "        word_importance_list = []\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_list.append(simple_model.coef_[i])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Selection rates: {np.mean(X, axis=0)}\")\n",
    "        print(f\"Outcome mean: {np.mean(Y):0.4f}\")\n",
    "        print(f\"Model MSE: {simple_model.score(X, Y):0.4f}\")\n",
    "    print(f\"Model MAPE: {mean_absolute_percentage_error(Y, simple_model.predict(X)):0.4f}\")\n",
    "  \n",
    "    if return_type == \"table\":\n",
    "        return word_importance_df\n",
    "    elif return_type == \"list\":\n",
    "        return all_words, word_importance_list\n",
    "    else:\n",
    "        return word_importance_df, (all_words, word_importance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_multiple_masking_score(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    n_samples_per_word = 5,\n",
    "    return_type=\"table\", # table, list, or both\n",
    "    ignore_first_x_words=0,\n",
    "  ):\n",
    "  # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    n_samples = len(all_words) * n_samples_per_word\n",
    "    word_scores = defaultdict(list)\n",
    "    X = []\n",
    "    # replacement_size = int(np.sqrt(len(all_words)) + 1)\n",
    "    replacement_size = int(len(all_words) * 0.15 + 1)\n",
    "    sentences = []\n",
    "    for _ in tqdm(range(n_samples), total = n_samples):\n",
    "        # Sample masking indices\n",
    "        word_indices = np.random.choice(\n",
    "            range(ignore_first_x_words, len(all_words)), \n",
    "            size=replacement_size,\n",
    "            replace = False,\n",
    "        )\n",
    "        current_x_row = np.ones(len(all_words))\n",
    "        for i in word_indices:\n",
    "            current_x_row[i] = 0\n",
    "        X.append(current_x_row)\n",
    "        words = [all_words[i] for i in word_indices]\n",
    "        words = [letter_regex.sub(\"\", word).lower() for word in words]\n",
    "        new_sentence = \" \".join([temp_word if j not in word_indices else \"\" for (j, temp_word) in enumerate(all_words)])\n",
    "        sentences.append(new_sentence)\n",
    "  \n",
    "    # Get model outcomes\n",
    "    dataset = tokenize_for_bert_classifier(\n",
    "      pd.DataFrame({\n",
    "          \"text\": sentences,\n",
    "          \"label\": [True for _ in sentences]\n",
    "      })\n",
    "    )\n",
    "    Y = classifier.predict(dataset).logits\n",
    "\n",
    "    # Train a simple model on the local data\n",
    "    simple_model = Lasso(lasso_alpha).fit(X, Y)\n",
    "\n",
    "    if return_type == \"table\" or return_type == \"both\":\n",
    "        filtered_words = list(filter(lambda w: w.lower() not in all_stopwords, all_words))\n",
    "        all_words_unique = [letter_regex.sub(\"\", word).lower() for word in filtered_words]\n",
    "        all_words_unique = list(set(all_words_unique))\n",
    "        word_importance_raw = defaultdict(list)\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_raw[letter_regex.sub(\"\", word).lower()].append(simple_model.coef_[i])\n",
    "        word_importance_df = pd.DataFrame(\n",
    "            {\n",
    "                \"word\": all_words_unique,\n",
    "                \"importance\": [np.mean(word_importance_raw[temp]) for temp in all_words_unique]\n",
    "            }\n",
    "        )\n",
    "        word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "  \n",
    "    if return_type == \"list\" or return_type == \"both\":\n",
    "        word_importance_list = []\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_list.append(simple_model.coef_[i])\n",
    "  \n",
    "    if verbose:\n",
    "        print(f\"Selection rates: {np.mean(X, axis=0)}\")\n",
    "        print(f\"Outcome mean: {np.mean(Y):0.4f}\")\n",
    "        print(f\"Model MSE: {simple_model.score(X, Y):0.4f}\")\n",
    "    print(f\"Model MAPE: {mean_absolute_percentage_error(Y, simple_model.predict(X)):0.4f}\")\n",
    "  \n",
    "    if return_type == \"table\":\n",
    "        return word_importance_df\n",
    "    elif return_type == \"list\":\n",
    "        return all_words, word_importance_list\n",
    "    else:\n",
    "        return word_importance_df, (all_words, word_importance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_color_style(word, score):\n",
    "    if score < 0.1 and score > -0.1:\n",
    "        return word\n",
    "    elif score >= 0.1 and score < 0.8:\n",
    "        return f'\\x1b[1;37;46m {word} \\x1b[0m'\n",
    "    elif score >= 0.8:\n",
    "        return f'\\x1b[1;37;42m {word} \\x1b[0m'\n",
    "    elif score <= -0.1 and score > -0.8:\n",
    "        return f'\\x1b[1;37;45m  {word} \\x1b[0m'\n",
    "    else:\n",
    "        return f'\\x1b[1;37;41m  {word} \\x1b[0m'\n",
    "\n",
    "def show_multi_masking_both_scores(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    gap_models = [gap_untuned_model],\n",
    "    descriptions = [\"Replacement with un-tuned bert\"],\n",
    "    show_colored_text = False,\n",
    "    masking_sample_size = 25,\n",
    "    replacement_sample_size = 5,\n",
    "    ignore_first_x_words=0,\n",
    "  ):\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier_model(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    print(f\"Sentence score: {inv_logit(original_sentence_score):0.4F}\")\n",
    "    if not show_colored_text:\n",
    "        masking_df = show_multiple_masking_score(original_sentence, \n",
    "                                                 verbose, \n",
    "                                                 classifier,\n",
    "                                                 n_samples_per_word = masking_sample_size,\n",
    "                                                 ignore_first_x_words = ignore_first_x_words)\n",
    "        replacement_dfs = [\n",
    "            show_multiple_masking_replacement_score(\n",
    "                original_sentence, \n",
    "                verbose,\n",
    "                classifier,\n",
    "                gap_model,\n",
    "                n_samples_per_word = replacement_sample_size,\n",
    "                ignore_first_x_words = ignore_first_x_words)\n",
    "            for gap_model in gap_models\n",
    "        ]\n",
    "        print(\"Baseline:\")\n",
    "        display(masking_df.style.apply(get_color, axis=None))\n",
    "        for description, replacement_df in zip(descriptions, replacement_dfs):\n",
    "            print(description)\n",
    "            display(replacement_df.style.apply(get_color, axis=None))\n",
    "    else:\n",
    "        masking_df, (words, masking_list) = show_multiple_masking_score(\n",
    "            original_sentence, \n",
    "            verbose, \n",
    "            classifier,\n",
    "            return_type = \"both\",\n",
    "            n_samples_per_word = masking_sample_size,\n",
    "            ignore_first_x_words = ignore_first_x_words,\n",
    "            )\n",
    "        replacement_data = [\n",
    "            show_multiple_masking_replacement_score(\n",
    "                original_sentence, \n",
    "                verbose,\n",
    "                classifier,\n",
    "                gap_model,\n",
    "                return_type = \"both\",\n",
    "                n_samples_per_word = replacement_sample_size,\n",
    "                ignore_first_x_words = ignore_first_x_words,\n",
    "                )\n",
    "            for gap_model in gap_models\n",
    "        ]\n",
    "        replacement_dfs = [temp[0] for temp in replacement_data]\n",
    "        replacement_lists = [temp[1][1] for temp in replacement_data]\n",
    "        print(\"Baseline:\")\n",
    "        display(masking_df.style.apply(get_color, axis=None))\n",
    "        masking_sentence = ' '.join([\n",
    "            format_color_style(word, score) \n",
    "            for word, score in zip(words, masking_list)\n",
    "        ])\n",
    "        print(masking_sentence)\n",
    "\n",
    "        for description, replacement_df, replacement_list in zip(descriptions, \n",
    "                                                                   replacement_dfs, \n",
    "                                                                   replacement_lists\n",
    "                                                                   ):\n",
    "            print(description)\n",
    "            display(replacement_df.style.apply(get_color, axis=None))\n",
    "            replacement_sentence = ' '.join([\n",
    "                format_color_style(word, score) \n",
    "                for word, score in zip(words, replacement_list)\n",
    "            ])\n",
    "            print(replacement_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd62a34",
   "metadata": {},
   "source": [
    "## Positive Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358399b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16663a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(qadataset_test[0]['text'], verbose=True, show_colored_text = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(qadataset_test[200]['text'], verbose=True, show_colored_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73307d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37866128",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(qadataset_test[50]['text'], verbose=True, show_colored_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880d5d4",
   "metadata": {},
   "source": [
    "## Negative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc22b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(qadataset_test[-1]['text'], verbose=True, show_colored_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dda8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298409cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(qadataset_test[-2]['text'], verbose=True, show_colored_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qadataset_test[-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(qadataset_test[-100]['text'], verbose=True, show_colored_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d5387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
