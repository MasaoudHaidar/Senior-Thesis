{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267c4b6e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26efea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12beee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.22.2\n",
    "\n",
    "!pip install statsmodels\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "!pip install -U tensorflow==2.10 \n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f777bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm.autonotebook import tqdm\n",
    "import spacy\n",
    "import re\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, mean_absolute_percentage_error, r2_score, jaccard_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# specific machine learning functionality\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Transformers\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    TFBertForSequenceClassification, \n",
    "    TFBertForMaskedLM, \n",
    "    TFBertModel,\n",
    "    #create_optimizer,\n",
    "    #DataCollatorForLanguageModeling,\n",
    "    #PreTrainedTokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a74495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/Disable Eager Execution\n",
    "# Reference: https://www.tensorflow.org/guide/eager\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
    "# without building graphs\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dir = \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20b91a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2223aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization parameters\n",
    "classifier_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(classifier_name, do_lower_case=True)\n",
    "batch_size = 8 \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0352357",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization function\n",
    "def tokenize_for_bert_classifier(df, should_shuffle=False):\n",
    "    # Tokenization\n",
    "    X_tokenized = bert_tokenizer.batch_encode_plus(\n",
    "            df[\"text\"],\n",
    "            return_tensors='tf',\n",
    "            add_special_tokens = True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_attention_mask = True,\n",
    "            truncation='longest_first'\n",
    "    )\n",
    "    # Creating TF datasets\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((X_tokenized[\"input_ids\"],\n",
    "                                                   X_tokenized[\"token_type_ids\"],\n",
    "                                                   X_tokenized[\"attention_mask\"]), \n",
    "                                                  df[\"label\"]))\n",
    "    if should_shuffle:\n",
    "      buffer_train = len(df[\"text\"])\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_train)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe64a2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = TFBertForSequenceClassification.from_pretrained(word_dir + 'Senior Thesis models/model_classifier_bert_1/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a74c8e",
   "metadata": {},
   "source": [
    "# Single Masking Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e2adf",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8be381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "gap_untuned_model = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "to_keep = [\n",
    "    \"n't\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"no\",\n",
    "    'noone',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'nothing',\n",
    "    'n‘t',\n",
    "    'n’t',\n",
    "    'only',\n",
    "    'quite',\n",
    "    'really',\n",
    "    'serious',\n",
    "    'several',\n",
    "    'still',\n",
    "    'such',\n",
    "    'take',\n",
    "    'too',\n",
    "    'top',\n",
    "    'unless',\n",
    "    'various',\n",
    "    'very',\n",
    "    'well',\n",
    "]\n",
    "all_stopwords = [word for word in all_stopwords if not word in to_keep]\n",
    "all_stopwords = set(all_stopwords)\n",
    "letter_regex = re.compile('[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d0bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_replacement_options_and_score(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    gap_filler = gap_untuned_model,\n",
    "  ):\n",
    "    # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    word_scores = defaultdict(list)\n",
    "    for i, word in tqdm(enumerate(all_words), total = min(256, len(all_words))):\n",
    "        if i > 256:\n",
    "            break\n",
    "        word = letter_regex.sub(\"\", word)\n",
    "        word = word.lower()\n",
    "        if word in all_stopwords:\n",
    "            continue\n",
    "    \n",
    "    new_sentence = \" \".join([temp_word if j!=i else \"[MASK]\" for (j, temp_word) in enumerate(all_words)])\n",
    "    # new_sentence = original_sentence.replace(word, \"[MASK]\")\n",
    "    inputs = bert_tokenizer(new_sentence, return_tensors=\"tf\")\n",
    "    logits = gap_filler(**inputs).logits\n",
    "\n",
    "    # retrieve index of [MASK]\n",
    "    mask_token_index = tf.where((inputs.input_ids == bert_tokenizer.mask_token_id)[0])\n",
    "    selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "    \n",
    "    # get top predictions\n",
    "    predicted_token_ids = tf.math.top_k(selected_logits, 10).indices[0]\n",
    "    options = bert_tokenizer.decode(predicted_token_ids)\n",
    "\n",
    "    if verbose:\n",
    "        print(new_sentence)\n",
    "        print(options)\n",
    "\n",
    "    # get scores of those predictions\n",
    "    filled_sentences = []\n",
    "    for filler_word in options.split():\n",
    "        new_filled_sentece = original_sentence.replace(word, filler_word)\n",
    "        filled_sentences.append(new_filled_sentece)\n",
    "    \n",
    "    # compute word importance:\n",
    "    try:\n",
    "        inputs = bert_tokenizer(\n",
    "          filled_sentences, \n",
    "          return_tensors=\"tf\",\n",
    "          padding=True,\n",
    "          #max_length=256,\n",
    "          truncation=True\n",
    "        )\n",
    "        logits = classifier(**inputs).logits\n",
    "        current_word_score = original_sentence_score - np.mean(logits[:,0].numpy())\n",
    "        word_scores[word].append(current_word_score)\n",
    "        if verbose:\n",
    "            print(f\"Sentence Scores: {logits[:,0].numpy()}\")\n",
    "            print(f\"Overall Score: {(np.mean(logits[:,0].numpy())):.4f}\")\n",
    "            print(f\"Word: {word}\")\n",
    "            print(f\"Word importance: {current_word_score:.4f}\")\n",
    "            print()\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"Did not compute\")\n",
    "        word_importance_df = pd.DataFrame(\n",
    "          {\n",
    "              \"word\": word_scores.keys(),\n",
    "              \"importance\": [np.mean(temp) for temp in word_scores.values()]\n",
    "          }\n",
    "        )\n",
    "    word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "    return word_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_masking_score(\n",
    "    original_sentence, \n",
    "    verbose = False,\n",
    "    classifier = classifier_model,\n",
    "  ):\n",
    "    # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    word_scores = defaultdict(list)\n",
    "    for i, word in tqdm(enumerate(all_words), total = min(256, len(all_words))):\n",
    "        if i > 256:\n",
    "            break\n",
    "        word = letter_regex.sub(\"\", word)\n",
    "        word = word.lower()\n",
    "        if word in all_stopwords:\n",
    "            continue\n",
    "        new_sentence = \" \".join([temp_word if j!=i else \"\" for (j, temp_word) in enumerate(all_words)])\n",
    "        if verbose:\n",
    "            print(new_sentence)\n",
    "    \n",
    "    # compute word importance:\n",
    "    try:\n",
    "        inputs = bert_tokenizer(new_sentence, return_tensors=\"tf\")\n",
    "        logits = classifier(**inputs).logits\n",
    "        current_word_score = original_sentence_score - np.mean(logits[:,0].numpy())\n",
    "        word_scores[word].append(current_word_score)\n",
    "        if verbose:\n",
    "            print(f\"Sentence Scores: {logits[:,0].numpy()}\")\n",
    "            print(f\"Overall Score: {(np.mean(logits[:,0].numpy())):.4f}\")\n",
    "            print(f\"Word: {word}\")\n",
    "            print(f\"Word importance: {current_word_score:.4f}\")\n",
    "            print()\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"Did not compute\")\n",
    "        word_importance_df = pd.DataFrame(\n",
    "          {\n",
    "              \"word\": word_scores.keys(),\n",
    "              \"importance\": [np.mean(temp) for temp in word_scores.values()]\n",
    "          }\n",
    "        )\n",
    "    word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "    return word_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c62035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(df):\n",
    "    x = df.copy()\n",
    "    for i, row in df.iterrows():\n",
    "        if row[\"importance\"] > 0:\n",
    "            green_value = min(max(0, row[\"importance\"]*80), 256)\n",
    "            style = f'background-color: rgb({256 - green_value}, 256, {256 - green_value})'\n",
    "        else:\n",
    "            red_value = min(max(0, -row[\"importance\"]*80), 256)\n",
    "            style = f'background-color: rgb(256, {256 - red_value}, {256 - red_value})'\n",
    "        x.iloc[i] = style\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_both_scores(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    gap_models = [gap_untuned_model],\n",
    "    descriptions = [\"Replacement with un-tuned bert\"]\n",
    "  ):\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier_model(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    print(f\"Sentence score: {inv_logit(original_sentence_score):0.4F}\")\n",
    "    masking_df = show_masking_score(original_sentence, verbose, classifier)\n",
    "    replacement_dfs = [\n",
    "      show_replacement_options_and_score(original_sentence, \n",
    "                                         verbose,\n",
    "                                         classifier,\n",
    "                                         gap_model)\n",
    "      for gap_model in gap_models\n",
    "    ]\n",
    "    print(\"Baseline:\")\n",
    "    display(masking_df.style.apply(get_color, axis=None))\n",
    "    for description, replacement_df in zip(descriptions, replacement_dfs):\n",
    "        print(description)\n",
    "        display(replacement_df.style.apply(get_color, axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca491d3",
   "metadata": {},
   "source": [
    "## Simple Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883524a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"I really like this movie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"I really like this movie. I enjoyed it very much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02607e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"I really hate this movie. I really like this movie.\") # 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e014fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"I really like this movie. I really hate this movie.\") # 47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"I really hate this movie. I enjoyed it a lot.\") # 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"I went to watch the movie last week . Drove there with some friends. Loved the company but hated the movie.\") # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a986ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"I was excited for the movie and thought it might be great. I loved the original but the sequel sucks. I hated every part of it\") # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b795528",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"The movie was okay but not amazing . I was excited .\") # 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14668c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_both_scores(\"The movie was meh . Not worth all the talk \") # 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93af7b25",
   "metadata": {},
   "source": [
    "# Interpretation with Tuned Gap Filler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1379f83",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2587c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_tuned_model = TFBertForMaskedLM.from_pretrained(word_dir + 'Senior Thesis models/model_LM_bert_1/temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1f6f4",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_three_scores(original_sentence, verbose = False):\n",
    "    show_both_scores(\n",
    "        original_sentence,\n",
    "        verbose,\n",
    "        gap_models = [gap_untuned_model, gap_tuned_model],\n",
    "        descriptions = [\"Replacement with un-tuned bert\", \"Replacement with tuned bert\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a641fb5",
   "metadata": {},
   "source": [
    "## Single Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd41f5",
   "metadata": {},
   "source": [
    "### Mixed Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b767b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I really hate this movie. I really like this movie.\") # 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718cb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I really like this movie. I really hate this movie.\") # 47%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I really hate this movie. I enjoyed it a lot.\") # 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I went to watch the movie last week . Drove there with some friends . Loved the company but hated the movie .\") # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbdcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"The movie was okay but not amazing . I was excited .\") # 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"The movie was meh . Not worth all the talk \") # 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa7a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"The actions scenes were great and the story is not bad .\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f624e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"It is not bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I went to watch the movie last summer. I had some popcorn while watching. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33212a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I went to watch the movie last summer. I had some popcorn while watching. It was bad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db1fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I went to watch the movie last summer. I had some popcorn while watching. It was amazing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fc565",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I went to watch the movie last summer. I had some popcorn while watching. It was amazing.\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I went to watch the movie last summer. I had some popcorn while watching. \", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a476f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I went to watch the movie last summer. I had some popcorn while watching. Didn't have high expectations of it so wasn't disappointed. All in all it was worth 10 bucks but not more.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I think I am a good movie critic. I have watched every top 10 movie in the last few years in addition to some of the old traditional stuff. This movie was okay.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca453a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_replacement_options_and_score(\"I think I am a good movie critic. I have watched every top 10 movie in the last few years in addition to some of the old traditional stuff. This movie was okay.\", verbose=True, gap_filler = gap_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65e986",
   "metadata": {},
   "source": [
    "### Positive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I really like this movie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46758922",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I really like this movie. I enjoyed it very much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0475c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"Best movie I have ever seen. I enjoyed every minute of it. The ending was amazing and I love the actors.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a711fa",
   "metadata": {},
   "source": [
    "### Very negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"Worst movie I have ever seen. I hated every minute of it. The story was slow and the actors are bad.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f5d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"Worst movie I have ever seen. I hated every minute of it. The story was slow and the actors are not great.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a846fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_three_scores(\"I was excited for the movie and thought it might be great . I loved the original but the sequel sucks . I hated every part of it\") # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589e087",
   "metadata": {},
   "source": [
    "# Multi-Masking Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcf795",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alpha = 0.01\n",
    "top_k_words = 10\n",
    "def show_multiple_masking_replacement_score(\n",
    "        original_sentence, \n",
    "        verbose = False, \n",
    "        classifier = classifier_model,\n",
    "        gap_filler = gap_untuned_model,\n",
    "        n_samples_per_word = 2,\n",
    "        return_type=\"table\", # table, list, or both\n",
    "        ignore_first_x_words=0,\n",
    "    ):\n",
    "    # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    n_samples = len(all_words) * n_samples_per_word\n",
    "    word_scores = defaultdict(list)\n",
    "    X = []\n",
    "    Y = []\n",
    "    # replacement_size = int(np.sqrt(len(all_words)) + 1)\n",
    "    replacement_size = int(len(all_words) * 0.15 + 1)\n",
    "    sentences = []\n",
    "    for _ in tqdm(range(n_samples), total = n_samples):\n",
    "        # Sample masking indices\n",
    "        word_indices = np.random.choice(\n",
    "            range(ignore_first_x_words, len(all_words)), \n",
    "            size=replacement_size,\n",
    "            replace = False,\n",
    "        )\n",
    "        current_x_row = np.ones(len(all_words))\n",
    "        for i in word_indices:\n",
    "            current_x_row[i] = 0\n",
    "        for _ in range(top_k_words):\n",
    "            X.append(current_x_row)\n",
    "        words = [all_words[i] for i in word_indices]\n",
    "        words = [letter_regex.sub(\"\", word).lower() for word in words]\n",
    "        new_sentence = \" \".join([temp_word if j not in word_indices else \"[MASK]\" for (j, temp_word) in enumerate(all_words)])\n",
    "\n",
    "        # get gap filler logits\n",
    "        inputs = bert_tokenizer(new_sentence, return_tensors=\"tf\")\n",
    "        logits = gap_filler(**inputs).logits\n",
    "\n",
    "        # retrieve indices of [MASK]\n",
    "        mask_token_index = tf.where((inputs.input_ids == bert_tokenizer.mask_token_id)[0])\n",
    "        selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)\n",
    "\n",
    "        # get top predictions\n",
    "        predicted_token_ids = [tf.math.top_k(temp, top_k_words).indices for temp in selected_logits]\n",
    "        options = [bert_tokenizer.decode(temp) for temp in predicted_token_ids]\n",
    "        options = [temp.split() for temp in options]\n",
    "        options = [temp if len(temp) == top_k_words \n",
    "                   else temp + [\"\" for _ in range(top_k_words - len(temp))]\n",
    "                   for temp in options]\n",
    "\n",
    "        # get scores of those predictions\n",
    "        filled_sentences = [new_sentence for _ in range(top_k_words)]\n",
    "        for i in range(top_k_words):\n",
    "            for j in range(replacement_size):\n",
    "                filled_sentences[i] = filled_sentences[i].replace(\"[MASK]\", options[j][i], 1)\n",
    "            sentences.append(filled_sentences[i])\n",
    "    \n",
    "    # compute model outcomes:\n",
    "    dataset = tokenize_for_bert_classifier(\n",
    "      pd.DataFrame({\n",
    "          \"text\": sentences,\n",
    "          \"label\": [True for _ in sentences]\n",
    "      })\n",
    "    )\n",
    "    Y = classifier.predict(dataset).logits\n",
    "  \n",
    "    # Train a simple model on the local data\n",
    "    simple_model = Lasso(lasso_alpha).fit(X, Y)\n",
    "  \n",
    "    if return_type == \"table\" or return_type == \"both\":\n",
    "        filtered_words = list(filter(lambda w: w.lower() not in all_stopwords, all_words))\n",
    "        all_words_unique = [letter_regex.sub(\"\", word).lower() for word in filtered_words]\n",
    "        all_words_unique = list(set(all_words_unique))\n",
    "        word_importance_raw = defaultdict(list)\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_raw[letter_regex.sub(\"\", word).lower()].append(simple_model.coef_[i])\n",
    "        word_importance_df = pd.DataFrame(\n",
    "            {\n",
    "                \"word\": all_words_unique,\n",
    "                \"importance\": [np.mean(word_importance_raw[temp]) for temp in all_words_unique]\n",
    "            }\n",
    "        )\n",
    "        word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "\n",
    "    if return_type == \"list\" or return_type == \"both\":\n",
    "        word_importance_list = []\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_list.append(simple_model.coef_[i])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Selection rates: {np.mean(X, axis=0)}\")\n",
    "        print(f\"Outcome mean: {np.mean(Y):0.4f}\")\n",
    "        print(f\"Model MSE: {simple_model.score(X, Y):0.4f}\")\n",
    "    print(f\"Model MAPE: {mean_absolute_percentage_error(Y, simple_model.predict(X)):0.4f}\")\n",
    "  \n",
    "    if return_type == \"table\":\n",
    "        return word_importance_df\n",
    "    elif return_type == \"list\":\n",
    "        return all_words, word_importance_list\n",
    "    else:\n",
    "        return word_importance_df, (all_words, word_importance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_multiple_masking_score(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    n_samples_per_word = 5,\n",
    "    return_type=\"table\", # table, list, or both\n",
    "    ignore_first_x_words=0,\n",
    "  ):\n",
    "  # original sentence\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    if verbose:\n",
    "        print(original_sentence)\n",
    "        print(f\"Original score: {original_sentence_score}\")\n",
    "        print()\n",
    "\n",
    "    # modefied sentences\n",
    "    all_words = original_sentence.split()\n",
    "    n_samples = len(all_words) * n_samples_per_word\n",
    "    word_scores = defaultdict(list)\n",
    "    X = []\n",
    "    # replacement_size = int(np.sqrt(len(all_words)) + 1)\n",
    "    replacement_size = int(len(all_words) * 0.15 + 1)\n",
    "    sentences = []\n",
    "    for _ in tqdm(range(n_samples), total = n_samples):\n",
    "        # Sample masking indices\n",
    "        word_indices = np.random.choice(\n",
    "            range(ignore_first_x_words, len(all_words)), \n",
    "            size=replacement_size,\n",
    "            replace = False,\n",
    "        )\n",
    "        current_x_row = np.ones(len(all_words))\n",
    "        for i in word_indices:\n",
    "            current_x_row[i] = 0\n",
    "        X.append(current_x_row)\n",
    "        words = [all_words[i] for i in word_indices]\n",
    "        words = [letter_regex.sub(\"\", word).lower() for word in words]\n",
    "        new_sentence = \" \".join([temp_word if j not in word_indices else \"\" for (j, temp_word) in enumerate(all_words)])\n",
    "        sentences.append(new_sentence)\n",
    "  \n",
    "    # Get model outcomes\n",
    "    dataset = tokenize_for_bert_classifier(\n",
    "      pd.DataFrame({\n",
    "          \"text\": sentences,\n",
    "          \"label\": [True for _ in sentences]\n",
    "      })\n",
    "    )\n",
    "    Y = classifier.predict(dataset).logits\n",
    "\n",
    "    # Train a simple model on the local data\n",
    "    simple_model = Lasso(lasso_alpha).fit(X, Y)\n",
    "\n",
    "    if return_type == \"table\" or return_type == \"both\":\n",
    "        filtered_words = list(filter(lambda w: w.lower() not in all_stopwords, all_words))\n",
    "        all_words_unique = [letter_regex.sub(\"\", word).lower() for word in filtered_words]\n",
    "        all_words_unique = list(set(all_words_unique))\n",
    "        word_importance_raw = defaultdict(list)\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_raw[letter_regex.sub(\"\", word).lower()].append(simple_model.coef_[i])\n",
    "        word_importance_df = pd.DataFrame(\n",
    "            {\n",
    "                \"word\": all_words_unique,\n",
    "                \"importance\": [np.mean(word_importance_raw[temp]) for temp in all_words_unique]\n",
    "            }\n",
    "        )\n",
    "        word_importance_df = word_importance_df.sort_values(by=\"importance\", ignore_index=True)\n",
    "  \n",
    "    if return_type == \"list\" or return_type == \"both\":\n",
    "        word_importance_list = []\n",
    "        for i, word in enumerate(all_words):\n",
    "            word_importance_list.append(simple_model.coef_[i])\n",
    "  \n",
    "    if verbose:\n",
    "        print(f\"Selection rates: {np.mean(X, axis=0)}\")\n",
    "        print(f\"Outcome mean: {np.mean(Y):0.4f}\")\n",
    "        print(f\"Model MSE: {simple_model.score(X, Y):0.4f}\")\n",
    "    print(f\"Model MAPE: {mean_absolute_percentage_error(Y, simple_model.predict(X)):0.4f}\")\n",
    "  \n",
    "    if return_type == \"table\":\n",
    "        return word_importance_df\n",
    "    elif return_type == \"list\":\n",
    "        return all_words, word_importance_list\n",
    "    else:\n",
    "        return word_importance_df, (all_words, word_importance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_color_style(word, score):\n",
    "    if score < 0.1 and score > -0.1:\n",
    "        return word\n",
    "    elif score >= 0.1 and score < 0.8:\n",
    "        return f'\\x1b[1;37;46m {word} \\x1b[0m'\n",
    "    elif score >= 0.8:\n",
    "        return f'\\x1b[1;37;42m {word} \\x1b[0m'\n",
    "    elif score <= -0.1 and score > -0.8:\n",
    "        return f'\\x1b[1;37;45m  {word} \\x1b[0m'\n",
    "    else:\n",
    "        return f'\\x1b[1;37;41m  {word} \\x1b[0m'\n",
    "\n",
    "def show_multi_masking_both_scores(\n",
    "    original_sentence, \n",
    "    verbose = False, \n",
    "    classifier = classifier_model,\n",
    "    gap_models = [gap_untuned_model],\n",
    "    descriptions = [\"Replacement with un-tuned bert\"],\n",
    "    show_colored_text = False,\n",
    "    masking_sample_size = 25,\n",
    "    replacement_sample_size = 5,\n",
    "    ignore_first_x_words=0,\n",
    "  ):\n",
    "    inputs = bert_tokenizer(original_sentence, return_tensors=\"tf\")\n",
    "    logits = classifier_model(**inputs).logits\n",
    "    original_sentence_score = logits[0,0].numpy()\n",
    "    print(f\"Sentence score: {inv_logit(original_sentence_score):0.4F}\")\n",
    "    if not show_colored_text:\n",
    "        masking_df = show_multiple_masking_score(original_sentence, \n",
    "                                                 verbose, \n",
    "                                                 classifier,\n",
    "                                                 n_samples_per_word = masking_sample_size,\n",
    "                                                 ignore_first_x_words = ignore_first_x_words)\n",
    "        replacement_dfs = [\n",
    "            show_multiple_masking_replacement_score(\n",
    "                original_sentence, \n",
    "                verbose,\n",
    "                classifier,\n",
    "                gap_model,\n",
    "                n_samples_per_word = replacement_sample_size,\n",
    "                ignore_first_x_words = ignore_first_x_words)\n",
    "            for gap_model in gap_models\n",
    "        ]\n",
    "        print(\"Baseline:\")\n",
    "        display(masking_df.style.apply(get_color, axis=None))\n",
    "        for description, replacement_df in zip(descriptions, replacement_dfs):\n",
    "            print(description)\n",
    "            display(replacement_df.style.apply(get_color, axis=None))\n",
    "    else:\n",
    "        masking_df, (words, masking_list) = show_multiple_masking_score(\n",
    "            original_sentence, \n",
    "            verbose, \n",
    "            classifier,\n",
    "            return_type = \"both\",\n",
    "            n_samples_per_word = masking_sample_size,\n",
    "            ignore_first_x_words = ignore_first_x_words,\n",
    "            )\n",
    "        replacement_data = [\n",
    "            show_multiple_masking_replacement_score(\n",
    "                original_sentence, \n",
    "                verbose,\n",
    "                classifier,\n",
    "                gap_model,\n",
    "                return_type = \"both\",\n",
    "                n_samples_per_word = replacement_sample_size,\n",
    "                ignore_first_x_words = ignore_first_x_words,\n",
    "                )\n",
    "            for gap_model in gap_models\n",
    "        ]\n",
    "        replacement_dfs = [temp[0] for temp in replacement_data]\n",
    "        replacement_lists = [temp[1][1] for temp in replacement_data]\n",
    "        print(\"Baseline:\")\n",
    "        display(masking_df.style.apply(get_color, axis=None))\n",
    "        masking_sentence = ' '.join([\n",
    "            format_color_style(word, score) \n",
    "            for word, score in zip(words, masking_list)\n",
    "        ])\n",
    "        print(masking_sentence)\n",
    "\n",
    "        for description, replacement_df, replacement_list in zip(descriptions, \n",
    "                                                                   replacement_dfs, \n",
    "                                                                   replacement_lists\n",
    "                                                                   ):\n",
    "            print(description)\n",
    "            display(replacement_df.style.apply(get_color, axis=None))\n",
    "            replacement_sentence = ' '.join([\n",
    "                format_color_style(word, score) \n",
    "                for word, score in zip(words, replacement_list)\n",
    "            ])\n",
    "            print(replacement_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd62a34",
   "metadata": {},
   "source": [
    "## Mixed Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I really like this movie. I really hate this movie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I really hate this movie. I really like this movie.\") # 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd1c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I really hate this movie. I enjoyed it a lot.\") # 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I went to watch the movie last week. Drove there with some friends. Loved the company but hated the movie.\") # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"The movie was okay but not amazing. I was excited.\") # 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"The movie was meh. Not worth all the talk\") # 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe32ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"The action scenes were great and the story is not bad.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ac227",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"It is not bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad503d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I went to watch the movie last summer. I had some popcorn while watching. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24691eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I went to watch the movie last summer. I had some popcorn while watching. It was bad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b8878",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I went to watch the movie last summer. I had some popcorn while watching. It was amazing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f318db",
   "metadata": {},
   "source": [
    "## Very positive Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I really like this movie.\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0076f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I really like this movie. I enjoyed it very much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"Best movie I have ever seen. I enjoyed every minute of it. The ending was amazing and I love the actors.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2abfef",
   "metadata": {},
   "source": [
    "## Very negative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c34b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"Worst movie I have ever seen. I hated every minute of it. The story was slow and the actors are bad.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe4026",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"Worst movie I have ever seen. I hated every minute of it. The story was slow and the actors are not great.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a082f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multi_masking_both_scores(\"I was excited for the movie and thought it might be great . I loved the original but the sequel sucks . I hated every part of it\") # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae943f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
